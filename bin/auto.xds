#!/usr/bin/env python3

import sys
import time
import argparse
import subprocess
import re
from pathlib import Path


def tail(file_path: Path, sentinel="XDS-DONE", interval=0.1):
    """
    Generator function that tails a file and yields new lines as they are added.

    :param file_path: Path to the file to be tailed.
    :param sentinel: return if any line contains this text
    :param interval: Time to wait (in seconds) between checks for new lines.
    """

    # wait for file to exist
    while not file_path.exists():
        print(f'Waiting for {file_path} ...')
        time.sleep(1)

    with open(file_path, 'r') as file:
        # Go to the end of the file
        found = False
        while not found:
            text = file.readline()
            found = bool(re.search(rf'{sentinel}', text))
            yield text


def run_slurm_job(command: str, nodes: int, cpus: int, tasks: int, user: str, partition: str = 'batch', duration: str = '00:30:00'):
    """
    Submit a SLURM job script to the cluster.

    :param command: Command to run in the job script
    :param nodes: Number of nodes
    :param cpus: Number of CPUs per task
    :param tasks: Total Number of tasks per node
    :param user: slurm user and hostname like 'username@hostname'
    :param duration: maximum duration of slurm job
    :param partition: slurm partition
    """

    # Generate the SLURM job script
    log_file = Path("xds-slurm.log")
    log_file.unlink(missing_ok=True)
    job_script = (
        "#!/bin/bash \n\n"
        f"#SBATCH --partition={partition}\n"
        f"#SBATCH --nodes={nodes}\n"
        f"#SBATCH --ntasks-per-node={tasks//nodes}  # number of MPI processes\n"
        f"#SBATCH --cpus-per-task={cpus}            # number of OpenMP threads\n"
        "#SBATCH --mem=0                           # use all memory\n"
        f"#SBATCH --time={duration}\n"
        f"#SBATCH --output={log_file}\n\n"
        f"# run {command} \n"
        f"{command}\n"
        "echo 'XDS-DONE'\n\n"
    )

    # Write the job script to a file
    job_script_file = Path("xds-slurm.sh")
    with open(job_script_file, 'w') as file:
        file.write(job_script)

    current_dir = Path.cwd().resolve()
    ssh_command = f'ssh -x {user} "cd {current_dir}; sbatch {job_script_file}"'
    subprocess.run(ssh_command, shell=True)

    # tail log file until job to complete
    cmd_output = tail(log_file)
    for line in cmd_output:
        print(line, end="")


def run_standard_job(command: str):
    """
    Run a command on the local machine.

    :param command: Command to run
    :param user: slurm user and hostname like 'username@hostname'
    """
    subprocess.run(command, shell=True)


if __name__ == "__main__":
    # Set up command line arguments
    parser = argparse.ArgumentParser(description="Generate and submit a SLURM job script.")
    parser.add_argument("command", type=str, help="Command to run in the job script")
    parser.add_argument("--nodes", type=int, help="Number of nodes")
    parser.add_argument("--cpus", type=int, help="Number of CPUs per task")
    parser.add_argument("--tasks", type=int, help="Total Number of tasks per node")
    parser.add_argument("--user", type=str, help="Slurm SSH User eg username@hostname")

    args = parser.parse_args()
    if args.nodes and args.cpus and args.tasks and args.user:
        run_slurm_job(args.command, args.nodes, args.cpus, args.tasks, args.user)
    else:
        run_standard_job(args.command)

